{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('data/all.txt','r') as datafile:\n",
    "#     quotes = datafile.readlines()\n",
    "# list(sorted(set(quotes[0].split())))[:145]\n",
    "# words = list(sorted(set(quotes[0].split())))[138:]\n",
    "# words[0] = '0' \n",
    "# words[1] = 'a'\n",
    "# word_to_int = dict([(w,i) for i,w in enumerate(words)])\n",
    "# int_to_word = dict([(i,w) for i,w in enumerate(words)])\n",
    "# word_to_int, int_to_word\n",
    "# len(int_to_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import re\n",
    "# strip_special_chars = re.compile(\"[^A-Za-z0-9., ]+\")\n",
    "\n",
    "# def cleanSentences(string):\n",
    "#     string = string.lower().replace(\"<br />\", \" \")\n",
    "#     sentence = re.sub(strip_special_chars, \"\", string.lower())\n",
    "#     return re.sub(r\"(\\w)([.,])\", r\"\\1 \\2\", sentence)\n",
    "\n",
    "\n",
    "# sentence = 'I will never be an old man. To me, old age is always 15 years older than I am.'\n",
    "\n",
    "# print(cleanSentences(sentence))\n",
    "\n",
    "# import pandas as pd\n",
    "# data = pd.read_csv('quotes_all.csv',delimiter=';')\n",
    "# topics = ['death' , 'family', 'freedom' , 'funny', 'happiness', 'life' , 'love', 'politics', 'success', 'science']\n",
    "\n",
    "# for topic in topics:\n",
    "#     quotes_data = data[data['Topic'].isin([topic])]\n",
    "\n",
    "#     with open('%s-punctuation.txt'%topic,'w+') as quotefile:\n",
    "#         for quote in quotes_data['Quote']:\n",
    "#             quotefile.writelines(cleanSentences(quote.lower()) + \" \")\n",
    "\n",
    "# quotes_data = data[data['Topic'].isin(topics)]\n",
    "\n",
    "# with open('all-punctuation.txt','w+') as quotefile:\n",
    "#     for quote in quotes_data['Quote']:\n",
    "#         quotefile.writelines(cleanSentences(quote.lower()) + \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Activation\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import LambdaCallback\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/all-punctuation.txt','r') as quotefile:\n",
    "    quotes = quotefile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/death.txt','r') as deathfile:\n",
    "    deathquotes = deathfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tokenizer(filters='')\n",
    "t.fit_on_texts(quotes)\n",
    "vocab_size = len(t.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13668,\n",
       " [',',\n",
       "  ',000',\n",
       "  ',000seat',\n",
       "  ',400',\n",
       "  '.',\n",
       "  '.,',\n",
       "  '...',\n",
       "  '.4',\n",
       "  '.5',\n",
       "  '.a',\n",
       "  '.c',\n",
       "  '.d',\n",
       "  '.j',\n",
       "  '.k',\n",
       "  '.m',\n",
       "  '.outside',\n",
       "  '.r',\n",
       "  '.s',\n",
       "  '.t',\n",
       "  '.that',\n",
       "  '.u',\n",
       "  '0',\n",
       "  '04',\n",
       "  '1',\n",
       "  '10',\n",
       "  '100',\n",
       "  '101',\n",
       "  '102',\n",
       "  '105',\n",
       "  '11',\n",
       "  '12',\n",
       "  '125',\n",
       "  '13',\n",
       "  '132',\n",
       "  '137',\n",
       "  '14',\n",
       "  '15',\n",
       "  '16',\n",
       "  '1600',\n",
       "  '17',\n",
       "  '17th',\n",
       "  '18',\n",
       "  '180',\n",
       "  '1832',\n",
       "  '187',\n",
       "  '18karat',\n",
       "  '18th',\n",
       "  '19',\n",
       "  '1911',\n",
       "  '1937',\n",
       "  '1948',\n",
       "  '1950',\n",
       "  '1950s',\n",
       "  '1960',\n",
       "  '1960s',\n",
       "  '1965',\n",
       "  '1967',\n",
       "  '1969',\n",
       "  '1970',\n",
       "  '1970s',\n",
       "  '1980s',\n",
       "  '1984',\n",
       "  '1988',\n",
       "  '1989',\n",
       "  '19th',\n",
       "  '19thcentury',\n",
       "  '19year',\n",
       "  '1st',\n",
       "  '1yearold',\n",
       "  '2',\n",
       "  '20',\n",
       "  '200',\n",
       "  '2000',\n",
       "  '2001',\n",
       "  '2008',\n",
       "  '200million',\n",
       "  '2010',\n",
       "  '2025',\n",
       "  '20million',\n",
       "  '20s',\n",
       "  '20th',\n",
       "  '20thcentury',\n",
       "  '21',\n",
       "  '215',\n",
       "  '21stcentury',\n",
       "  '22',\n",
       "  '22year',\n",
       "  '24',\n",
       "  '247',\n",
       "  '25',\n",
       "  '250',\n",
       "  '26',\n",
       "  '28',\n",
       "  '3',\n",
       "  '30',\n",
       "  '300',\n",
       "  '30th',\n",
       "  '31',\n",
       "  '32',\n",
       "  '33',\n",
       "  '37',\n",
       "  '3d',\n",
       "  '3yearold',\n",
       "  '4',\n",
       "  '40',\n",
       "  '400',\n",
       "  '40hour',\n",
       "  '40th',\n",
       "  '42nd',\n",
       "  '45',\n",
       "  '48',\n",
       "  '4th',\n",
       "  '5',\n",
       "  '50',\n",
       "  '500',\n",
       "  '50s',\n",
       "  '50yearsold',\n",
       "  '56',\n",
       "  '58',\n",
       "  '6',\n",
       "  '60',\n",
       "  '60s',\n",
       "  '60yearold',\n",
       "  '62',\n",
       "  '67',\n",
       "  '6th',\n",
       "  '7',\n",
       "  '70',\n",
       "  '700page',\n",
       "  '70s',\n",
       "  '73',\n",
       "  '8',\n",
       "  '80',\n",
       "  '800',\n",
       "  '80s',\n",
       "  '88',\n",
       "  '9',\n",
       "  '90',\n",
       "  '9000',\n",
       "  '90s',\n",
       "  '91',\n",
       "  '911',\n",
       "  '92',\n",
       "  '96',\n",
       "  '99',\n",
       "  '9k',\n",
       "  'a',\n",
       "  'aaa',\n",
       "  'ababa',\n",
       "  'aback',\n",
       "  'abandon',\n",
       "  'abandoned',\n",
       "  'abandonment',\n",
       "  'abates',\n",
       "  'abba',\n",
       "  'abbreviating',\n",
       "  'abc',\n",
       "  'abdicate',\n",
       "  'abdicated',\n",
       "  'abductions',\n",
       "  'aberration',\n",
       "  'abiding',\n",
       "  'abilities',\n",
       "  'ability',\n",
       "  'abject',\n",
       "  'abjure',\n",
       "  'able',\n",
       "  'ably',\n",
       "  'abnormal',\n",
       "  'abnormality',\n",
       "  'abolish',\n",
       "  'abolished',\n",
       "  'abolition',\n",
       "  'abolitionists',\n",
       "  'abortion',\n",
       "  'abortions',\n",
       "  'about',\n",
       "  'above',\n",
       "  'abraham',\n",
       "  'abridgement',\n",
       "  'abroad',\n",
       "  'abrupt',\n",
       "  'absence',\n",
       "  'absences',\n",
       "  'absent',\n",
       "  'absolute',\n",
       "  'absolutely',\n",
       "  'absolutes',\n",
       "  'absorb',\n",
       "  'absorbed',\n",
       "  'abstract',\n",
       "  'abstractly',\n",
       "  'absurd',\n",
       "  'absurdist',\n",
       "  'absurdity',\n",
       "  'absurdly',\n",
       "  'abundance',\n",
       "  'abundant',\n",
       "  'abundantly',\n",
       "  'abuse',\n",
       "  'abuses',\n",
       "  'abusive',\n",
       "  'abyss',\n",
       "  'academia',\n",
       "  'academic',\n",
       "  'academies',\n",
       "  'academy',\n",
       "  'accelerate',\n",
       "  'accentuate',\n",
       "  'accentuated',\n",
       "  'accept',\n",
       "  'acceptable',\n",
       "  'acceptance',\n",
       "  'accepted',\n",
       "  'accepting',\n",
       "  'accepts',\n",
       "  'access',\n",
       "  'accessible',\n",
       "  'accident',\n",
       "  'accidental',\n",
       "  'accidentally',\n",
       "  'accidents',\n",
       "  'acclimate',\n",
       "  'accolades',\n",
       "  'accommodate',\n",
       "  'accompanied',\n",
       "  'accompanies',\n",
       "  'accomplish',\n",
       "  'accomplished',\n",
       "  'accomplishes',\n",
       "  'accomplishing',\n",
       "  'accomplishment',\n",
       "  'accomplishments',\n",
       "  'accord',\n",
       "  'according',\n",
       "  'accordingly',\n",
       "  'account',\n",
       "  'accountability',\n",
       "  'accountable',\n",
       "  'accountancy',\n",
       "  'accountant',\n",
       "  'accounts',\n",
       "  'accretion',\n",
       "  'accumulates',\n",
       "  'accumulation',\n",
       "  'accuracy',\n",
       "  'accurate',\n",
       "  'accurately',\n",
       "  'accuse',\n",
       "  'accusing',\n",
       "  'accustomed',\n",
       "  'acheive',\n",
       "  'aches',\n",
       "  'achievable',\n",
       "  'achieve',\n",
       "  'achieved',\n",
       "  'achievement',\n",
       "  'achievements',\n",
       "  'achieves',\n",
       "  'achieving',\n",
       "  'acids',\n",
       "  'acknowledge',\n",
       "  'acknowledged',\n",
       "  'acknowledgement',\n",
       "  'acknowledges',\n",
       "  'acknowledging',\n",
       "  'acquaintance',\n",
       "  'acquaintances',\n",
       "  'acquainted',\n",
       "  'acquiesce',\n",
       "  'acquire',\n",
       "  'acquired',\n",
       "  'acquiring',\n",
       "  'acquisition',\n",
       "  'acquisitiveness',\n",
       "  'acquits',\n",
       "  'acres',\n",
       "  'acrobat',\n",
       "  'across',\n",
       "  'acrosstheboard',\n",
       "  'act',\n",
       "  'acted',\n",
       "  'acting',\n",
       "  'action',\n",
       "  'actionable',\n",
       "  'actions',\n",
       "  'active',\n",
       "  'actively',\n",
       "  'activist',\n",
       "  'activists',\n",
       "  'activities',\n",
       "  'activity',\n",
       "  'actor',\n",
       "  'actors',\n",
       "  'actress',\n",
       "  'actresss',\n",
       "  'acts',\n",
       "  'actual',\n",
       "  'actuality',\n",
       "  'actually',\n",
       "  'acute',\n",
       "  'acutely',\n",
       "  'ad',\n",
       "  'adam',\n",
       "  'adapt',\n",
       "  'adapted',\n",
       "  'add',\n",
       "  'added',\n",
       "  'addict',\n",
       "  'addicted',\n",
       "  'addiction',\n",
       "  'addictions',\n",
       "  'addicts',\n",
       "  'adding',\n",
       "  'addis',\n",
       "  'addition',\n",
       "  'additional',\n",
       "  'additions',\n",
       "  'address',\n",
       "  'addressed',\n",
       "  'adds',\n",
       "  'adept',\n",
       "  'adequate',\n",
       "  'adequately',\n",
       "  'adherence',\n",
       "  'adherents',\n",
       "  'adjust',\n",
       "  'adjusting',\n",
       "  'adjustment',\n",
       "  'administered',\n",
       "  'administration',\n",
       "  'administrations',\n",
       "  'administratively',\n",
       "  'admirable',\n",
       "  'admiration',\n",
       "  'admire',\n",
       "  'admired',\n",
       "  'admires',\n",
       "  'admission',\n",
       "  'admit',\n",
       "  'admonish',\n",
       "  'adolescence',\n",
       "  'adolescent',\n",
       "  'adopt',\n",
       "  'adopted',\n",
       "  'adopting',\n",
       "  'adoption',\n",
       "  'adopts',\n",
       "  'adorable',\n",
       "  'adoration',\n",
       "  'adore',\n",
       "  'adored',\n",
       "  'adorers',\n",
       "  'adorned',\n",
       "  'adrenalin',\n",
       "  'adriatic',\n",
       "  'ads',\n",
       "  'adult',\n",
       "  'adulteration',\n",
       "  'adultery',\n",
       "  'adults',\n",
       "  'advance',\n",
       "  'advanced',\n",
       "  'advancement',\n",
       "  'advancements',\n",
       "  'advances',\n",
       "  'advancing',\n",
       "  'advantage',\n",
       "  'advantages',\n",
       "  'adventure',\n",
       "  'adventured',\n",
       "  'adventures',\n",
       "  'adventurous',\n",
       "  'adversaries',\n",
       "  'adversity',\n",
       "  'advertisement',\n",
       "  'advertisements',\n",
       "  'advertising',\n",
       "  'advice',\n",
       "  'advise',\n",
       "  'advised',\n",
       "  'advising',\n",
       "  'advisor',\n",
       "  'advocate',\n",
       "  'advocates',\n",
       "  'advocating',\n",
       "  'aegean',\n",
       "  'aerodynamically',\n",
       "  'aesthetic',\n",
       "  'aesthetics',\n",
       "  'afer',\n",
       "  'affair',\n",
       "  'affairs',\n",
       "  'affect',\n",
       "  'affectation',\n",
       "  'affected',\n",
       "  'affection',\n",
       "  'affections',\n",
       "  'affects',\n",
       "  'affiliation',\n",
       "  'affirmation',\n",
       "  'affirmative',\n",
       "  'affirmed',\n",
       "  'affixed',\n",
       "  'afflicted',\n",
       "  'affliction',\n",
       "  'afflicts',\n",
       "  'afford',\n",
       "  'afforded',\n",
       "  'affords',\n",
       "  'affront',\n",
       "  'afghan',\n",
       "  'afghanistan',\n",
       "  'aficionados',\n",
       "  'aflame',\n",
       "  'afloat',\n",
       "  'afraid',\n",
       "  'africa',\n",
       "  'african',\n",
       "  'africanamerican',\n",
       "  'africanamericans',\n",
       "  'africans',\n",
       "  'africas',\n",
       "  'afro',\n",
       "  'after',\n",
       "  'afterglow',\n",
       "  'afterlife',\n",
       "  'aftermath',\n",
       "  'afternoon',\n",
       "  'afterthought',\n",
       "  'afterward',\n",
       "  'afterwards',\n",
       "  'again',\n",
       "  'against',\n",
       "  'age',\n",
       "  'aged',\n",
       "  'agency',\n",
       "  'agenda',\n",
       "  'agent',\n",
       "  'agents',\n",
       "  'ages',\n",
       "  'aggravated',\n",
       "  'aggravating',\n",
       "  'aggregate',\n",
       "  'aggression',\n",
       "  'aggressive',\n",
       "  'aggressiveness',\n",
       "  'aging',\n",
       "  'agitation',\n",
       "  'agnostic',\n",
       "  'ago',\n",
       "  'agonising',\n",
       "  'agonizing',\n",
       "  'agony',\n",
       "  'agree',\n",
       "  'agreeable',\n",
       "  'agreed',\n",
       "  'agreement',\n",
       "  'agricultural',\n",
       "  'agriculture',\n",
       "  'ah',\n",
       "  'ahead',\n",
       "  'aid',\n",
       "  'aide',\n",
       "  'aids',\n",
       "  'ailing',\n",
       "  'aim',\n",
       "  'aiming',\n",
       "  'aimless',\n",
       "  'aims',\n",
       "  'aint',\n",
       "  'air',\n",
       "  'aircraft',\n",
       "  'airline',\n",
       "  'airplane',\n",
       "  'airplanes',\n",
       "  'airports',\n",
       "  'airwaves',\n",
       "  'aisles',\n",
       "  'akin',\n",
       "  'al',\n",
       "  'alarm',\n",
       "  'alarmed',\n",
       "  'alarms',\n",
       "  'alas',\n",
       "  'albany',\n",
       "  'album',\n",
       "  'albums',\n",
       "  'alcohol',\n",
       "  'alcoholic',\n",
       "  'alcoholics',\n",
       "  'alert',\n",
       "  'alex',\n",
       "  'algebra',\n",
       "  'algorithms',\n",
       "  'ali',\n",
       "  'aliceera',\n",
       "  'alien',\n",
       "  'alienate',\n",
       "  'alienating',\n",
       "  'alienation',\n",
       "  'alight',\n",
       "  'align',\n",
       "  'alignment',\n",
       "  'alike',\n",
       "  'alive',\n",
       "  'all',\n",
       "  'alleged',\n",
       "  'allegedly',\n",
       "  'allegiance',\n",
       "  'allen',\n",
       "  'alleviating',\n",
       "  'allgreat',\n",
       "  'alliance',\n",
       "  'alliances',\n",
       "  'allied',\n",
       "  'allies',\n",
       "  'alljapanese',\n",
       "  'allot',\n",
       "  'allow',\n",
       "  'allowances',\n",
       "  'allowed',\n",
       "  'allowing',\n",
       "  'allows',\n",
       "  'allpowerful',\n",
       "  'alltogether',\n",
       "  'allures',\n",
       "  'allwhite',\n",
       "  'allwise',\n",
       "  'alma',\n",
       "  'almighty',\n",
       "  'almost',\n",
       "  'almostinevitable',\n",
       "  'alms',\n",
       "  'alone',\n",
       "  'along',\n",
       "  'alongside',\n",
       "  'aloof',\n",
       "  'alphabetically',\n",
       "  'already',\n",
       "  'alright',\n",
       "  'also',\n",
       "  'altar',\n",
       "  'alter',\n",
       "  'alteration',\n",
       "  'altered',\n",
       "  'altering',\n",
       "  'alternate',\n",
       "  'alternative',\n",
       "  'alternatives',\n",
       "  'alters',\n",
       "  'although',\n",
       "  'altogether',\n",
       "  'always',\n",
       "  'alzheimers',\n",
       "  'am',\n",
       "  'amateur',\n",
       "  'amazed',\n",
       "  'amazement',\n",
       "  'amazing',\n",
       "  'amazingly',\n",
       "  'amazon',\n",
       "  'ambassador',\n",
       "  'ambassadors',\n",
       "  'ambience',\n",
       "  'ambiguity',\n",
       "  'ambition',\n",
       "  'ambitions',\n",
       "  'ambitious',\n",
       "  'ambush',\n",
       "  'amending',\n",
       "  'amendment',\n",
       "  'amendments',\n",
       "  'amenities',\n",
       "  'america',\n",
       "  'american',\n",
       "  'americans',\n",
       "  'americas',\n",
       "  'amiable',\n",
       "  'amid',\n",
       "  'amidst',\n",
       "  'amin',\n",
       "  'amino',\n",
       "  'amis',\n",
       "  'amnesia',\n",
       "  'amnesty',\n",
       "  'among',\n",
       "  'amongst',\n",
       "  'amount',\n",
       "  'amounts',\n",
       "  'amp',\n",
       "  'amplify',\n",
       "  'amply',\n",
       "  'amulet',\n",
       "  'amuse',\n",
       "  'amused',\n",
       "  'amusements',\n",
       "  'amusing',\n",
       "  'an',\n",
       "  'analogy',\n",
       "  'analyse',\n",
       "  'analysis',\n",
       "  'analyst',\n",
       "  'analytical',\n",
       "  'analytically',\n",
       "  'analyze',\n",
       "  'analyzing',\n",
       "  'anarchist',\n",
       "  'anarchists',\n",
       "  'anarchy',\n",
       "  'anathema',\n",
       "  'anatidae',\n",
       "  'ancestor',\n",
       "  'ancestors',\n",
       "  'ancestral',\n",
       "  'ancestry',\n",
       "  'anchored',\n",
       "  'anchors',\n",
       "  'ancient',\n",
       "  'ancients',\n",
       "  'and',\n",
       "  'andor',\n",
       "  'andrew',\n",
       "  'andy',\n",
       "  'anecdotal',\n",
       "  'anesthesia',\n",
       "  'anew',\n",
       "  'angel',\n",
       "  'angeles',\n",
       "  'angelic',\n",
       "  'angels',\n",
       "  'anger',\n",
       "  'angered',\n",
       "  'angle',\n",
       "  'anglican',\n",
       "  'anglocatholic',\n",
       "  'angry',\n",
       "  'anguish',\n",
       "  'anguished',\n",
       "  'animal',\n",
       "  'animals',\n",
       "  'animate',\n",
       "  'animated',\n",
       "  'animation',\n",
       "  'animosity',\n",
       "  'anna',\n",
       "  'anne',\n",
       "  'annihilate',\n",
       "  'annihilating',\n",
       "  'annihilation',\n",
       "  'anniversary',\n",
       "  'announce',\n",
       "  'announced',\n",
       "  'annoy',\n",
       "  'annoyance',\n",
       "  'annoying',\n",
       "  'annually',\n",
       "  'anonymity',\n",
       "  'anonymous',\n",
       "  'anonymously',\n",
       "  'another',\n",
       "  'anothers',\n",
       "  'answer',\n",
       "  'answerable',\n",
       "  'answered',\n",
       "  'answers',\n",
       "  'antagonism',\n",
       "  'anterior',\n",
       "  'anthem',\n",
       "  'anthropologists',\n",
       "  'anthropology',\n",
       "  'anti',\n",
       "  'anticipate',\n",
       "  'anticipated',\n",
       "  'anticipating',\n",
       "  'anticipation',\n",
       "  'anticipations',\n",
       "  'antidote',\n",
       "  'antigay',\n",
       "  'antipathy',\n",
       "  'antisemitism',\n",
       "  'antithesis',\n",
       "  'antithetical',\n",
       "  'antoinette',\n",
       "  'anvil',\n",
       "  'anxiety',\n",
       "  'anxious',\n",
       "  'any',\n",
       "  'anybody',\n",
       "  'anybodys',\n",
       "  'anymore',\n",
       "  'anyone',\n",
       "  'anyones',\n",
       "  'anyplace',\n",
       "  'anything',\n",
       "  'anythings',\n",
       "  'anytime',\n",
       "  'anyway',\n",
       "  'anywayive',\n",
       "  'anywhere',\n",
       "  'ap',\n",
       "  'apart',\n",
       "  'apartheid',\n",
       "  'apartment',\n",
       "  'apathy',\n",
       "  'ape',\n",
       "  'aphrodisiacal',\n",
       "  'apocalypse',\n",
       "  'apocalyptic',\n",
       "  'apolitical',\n",
       "  'apollo',\n",
       "  'apologize',\n",
       "  'apologizing',\n",
       "  'apostle',\n",
       "  'appalled',\n",
       "  'appalling',\n",
       "  'apparent',\n",
       "  'apparently',\n",
       "  'appeal',\n",
       "  'appealed',\n",
       "  'appealing',\n",
       "  'appeals',\n",
       "  'appear',\n",
       "  'appearance',\n",
       "  'appearances',\n",
       "  'appearences',\n",
       "  'appearing',\n",
       "  'appears',\n",
       "  'appetite',\n",
       "  'appetites',\n",
       "  'applaud',\n",
       "  'applauded',\n",
       "  'applauds',\n",
       "  'applause',\n",
       "  'apple',\n",
       "  'apples',\n",
       "  'applesauce',\n",
       "  'application',\n",
       "  'applications',\n",
       "  'applied',\n",
       "  'applies',\n",
       "  'apply',\n",
       "  'applying',\n",
       "  'appoint',\n",
       "  'appointed',\n",
       "  'appointment',\n",
       "  'appointments',\n",
       "  'appreciate',\n",
       "  'appreciated',\n",
       "  'appreciating',\n",
       "  'appreciation',\n",
       "  'appreciative',\n",
       "  'apprehend',\n",
       "  'apprehension',\n",
       "  'approach',\n",
       "  'approached',\n",
       "  'approaches',\n",
       "  'approaching',\n",
       "  'appropriate',\n",
       "  'approval',\n",
       "  'approve',\n",
       "  'approved',\n",
       "  'approves',\n",
       "  'approximately',\n",
       "  'apt',\n",
       "  'aptitude',\n",
       "  'aptitudes',\n",
       "  'aquatic',\n",
       "  'arab',\n",
       "  'arabia',\n",
       "  'arabic',\n",
       "  'arabs',\n",
       "  'arbitrary',\n",
       "  'arbitration',\n",
       "  'arc',\n",
       "  'arcane',\n",
       "  'archaeological',\n",
       "  'archaeology',\n",
       "  'arched',\n",
       "  'archetypal',\n",
       "  'archetype',\n",
       "  'architect',\n",
       "  'architects',\n",
       "  'architecture',\n",
       "  'architectures',\n",
       "  'arctic',\n",
       "  'ardently',\n",
       "  'arduous',\n",
       "  'are',\n",
       "  'area',\n",
       "  'areas',\n",
       "  'arena',\n",
       "  'arent',\n",
       "  'argentina',\n",
       "  'arguably',\n",
       "  'argue',\n",
       "  'arguing',\n",
       "  'argument',\n",
       "  'arguments',\n",
       "  'aright',\n",
       "  'arise',\n",
       "  'arisen',\n",
       "  'arises',\n",
       "  'arising',\n",
       "  'aristotle',\n",
       "  'aristotles',\n",
       "  'arizona',\n",
       "  'arkansas',\n",
       "  'arm',\n",
       "  'armaments',\n",
       "  'armed',\n",
       "  'armenian',\n",
       "  'armies',\n",
       "  'armor',\n",
       "  'armour',\n",
       "  'armoured',\n",
       "  'arms',\n",
       "  'army',\n",
       "  'aronofsky',\n",
       "  'arose',\n",
       "  'around',\n",
       "  'aroused',\n",
       "  'arrange',\n",
       "  'arranged',\n",
       "  'arrangement',\n",
       "  'arranger',\n",
       "  'array',\n",
       "  'arresting',\n",
       "  'arrival',\n",
       "  'arrive',\n",
       "  'arrived',\n",
       "  'arrives',\n",
       "  'arriving',\n",
       "  'arrogance',\n",
       "  'arrogant',\n",
       "  'arrogantly',\n",
       "  'arrow',\n",
       "  'arrows',\n",
       "  'art',\n",
       "  'arteries',\n",
       "  'artforms',\n",
       "  'arthritis',\n",
       "  'arthurs',\n",
       "  'artichokes',\n",
       "  'article',\n",
       "  'articulate',\n",
       "  'artifact',\n",
       "  'artificial',\n",
       "  'artist',\n",
       "  'artistic',\n",
       "  'artistry',\n",
       "  'artists',\n",
       "  'arts',\n",
       "  'artwork',\n",
       "  'as',\n",
       "  'ascertained',\n",
       "  'ascribe',\n",
       "  'ascribed',\n",
       "  'ashamed',\n",
       "  'ashcroft',\n",
       "  'ashes',\n",
       "  'ashore',\n",
       "  'ashtray',\n",
       "  'asia',\n",
       "  'asian',\n",
       "  'aside',\n",
       "  'ask',\n",
       "  'asked',\n",
       "  'askew',\n",
       "  'asking',\n",
       "  'asks',\n",
       "  'asleep',\n",
       "  'aspect',\n",
       "  'aspects',\n",
       "  'aspiration',\n",
       "  'aspirational',\n",
       "  'aspirations',\n",
       "  'aspire',\n",
       "  'aspires',\n",
       "  'ass',\n",
       "  'assassin',\n",
       "  'assassinate',\n",
       "  'assassination',\n",
       "  'assassins',\n",
       "  'assault',\n",
       "  'assemble',\n",
       "  'assembling',\n",
       "  'assembly',\n",
       "  'assent',\n",
       "  'assert',\n",
       "  'asserting',\n",
       "  'assess',\n",
       "  'assessment',\n",
       "  'asset',\n",
       "  'assign',\n",
       "  'assigned',\n",
       "  'assignment',\n",
       "  'assignments',\n",
       "  'assimilate',\n",
       "  'assimilated',\n",
       "  'assimilation',\n",
       "  'assist',\n",
       "  'assistance',\n",
       "  'assistants',\n",
       "  'assisted',\n",
       "  'assisting',\n",
       "  'associate',\n",
       "  'associated',\n",
       "  'associates',\n",
       "  'association',\n",
       "  'associations',\n",
       "  'associative',\n",
       "  'assume',\n",
       "  'assumed',\n",
       "  'assumes',\n",
       "  'assumption',\n",
       "  'assumptions',\n",
       "  'assurances',\n",
       "  'assure',\n",
       "  'assured',\n",
       "  'assuredly',\n",
       "  'assures',\n",
       "  'astaire',\n",
       "  'astonished',\n",
       "  'astonishing',\n",
       "  'astonishment',\n",
       "  'astounding',\n",
       "  'astrobiology',\n",
       "  'astrology',\n",
       "  'astronomers',\n",
       "  'astronomy',\n",
       "  'astute',\n",
       "  'at',\n",
       "  'ate',\n",
       "  'athanasian',\n",
       "  'atheism',\n",
       "  'atheist',\n",
       "  'atheists',\n",
       "  'athena',\n",
       "  'athlete',\n",
       "  'athletes',\n",
       "  'athletic',\n",
       "  'athletics',\n",
       "  'atmosphere',\n",
       "  'atom',\n",
       "  'atomic',\n",
       "  'atoms',\n",
       "  'atrocities',\n",
       "  'attach',\n",
       "  'attached',\n",
       "  'attachment',\n",
       "  'attachments',\n",
       "  'attack',\n",
       "  'attacked',\n",
       "  'attacks',\n",
       "  'attain',\n",
       "  'attainable',\n",
       "  'attained',\n",
       "  'attaining',\n",
       "  'attainment',\n",
       "  'attains',\n",
       "  'attempt',\n",
       "  'attempted',\n",
       "  'attempting',\n",
       "  'attempts',\n",
       "  'attend',\n",
       "  'attended',\n",
       "  'attends',\n",
       "  'attention',\n",
       "  'attentive',\n",
       "  'attest',\n",
       "  'attic',\n",
       "  'attitude',\n",
       "  'attitudes',\n",
       "  'attorney',\n",
       "  'attorneys',\n",
       "  'attract',\n",
       "  'attracted',\n",
       "  'attracting',\n",
       "  'attraction',\n",
       "  'attractions',\n",
       "  'attractive',\n",
       "  'attracts',\n",
       "  'attributable',\n",
       "  'attribute',\n",
       "  'attributed',\n",
       "  'attributes',\n",
       "  'attribution',\n",
       "  'audacious',\n",
       "  'audacity',\n",
       "  'audibly',\n",
       "  'audience',\n",
       "  'audiences',\n",
       "  'audit',\n",
       "  'audition',\n",
       "  'auditioned',\n",
       "  'audrey',\n",
       "  'aught',\n",
       "  'augury',\n",
       "  'aunt',\n",
       "  ...])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size, sorted(t.word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_word = dict([(i,w) for (w,i) in t.word_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('index_word-punc.npy',index_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# integer encode the documents\n",
    "encoded_docs = t.texts_to_sequences(deathquotes)\n",
    "print(encoded_docs[0][:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = np.load('data/embeddings_index.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = embeddings_index.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ -1.08139999e-01,   3.58909994e-01,   8.85179996e-01,\n",
       "         -4.43589985e-01,  -5.94860017e-01,   3.36030006e-01,\n",
       "          2.20670000e-01,  -2.28680000e-01,   3.17539990e-01,\n",
       "         -3.80080014e-01,   6.47570014e-01,  -1.06610000e-01,\n",
       "          9.07329991e-02,   4.26490009e-02,   2.32319999e-03,\n",
       "         -8.50799978e-02,  -7.65439987e-01,   6.59240007e-01,\n",
       "         -1.16489995e+00,   7.28209972e-01,   2.49039993e-01,\n",
       "          5.30610010e-02,  -3.93390000e-01,  -2.65689999e-01,\n",
       "          5.13100028e-01,   7.51519978e-01,  -1.60030007e-01,\n",
       "         -4.55449998e-01,   5.58340013e-01,  -1.86780006e-01,\n",
       "          1.38100004e-03,   4.25550014e-01,   1.31119996e-01,\n",
       "         -2.21279994e-01,  -1.83630005e-01,  -4.20720018e-02,\n",
       "          1.64600000e-01,   8.76670033e-02,  -3.97680001e-03,\n",
       "         -5.45130014e-01,  -1.36419997e-01,   5.08280005e-03,\n",
       "          1.05789997e-01,  -3.45759988e-01,  -5.86000025e-01,\n",
       "         -8.03650022e-01,   8.87759961e-03,  -1.88800007e-01,\n",
       "         -4.31169987e-01,  -1.17560005e+00,   1.45300001e-01,\n",
       "          3.06609988e-01,  -3.14619988e-01,   1.24290001e+00,\n",
       "         -3.96840006e-01,  -2.18440008e+00,   5.78069985e-01,\n",
       "          7.58010021e-04,   1.40100002e+00,   4.42699999e-01,\n",
       "         -7.61839986e-01,   6.22139990e-01,  -1.04949999e+00,\n",
       "         -5.88599980e-01,   1.20379996e+00,   3.02019995e-02,\n",
       "          5.21589994e-01,   2.65960008e-01,   2.31439993e-02,\n",
       "         -7.34770000e-02,   5.32079995e-01,  -4.83779997e-01,\n",
       "         -1.35820001e-01,  -3.54090005e-01,   8.38410020e-01,\n",
       "          1.41969994e-01,   1.11790001e-02,   2.07839996e-01,\n",
       "         -7.64609993e-01,  -9.92100015e-02,   3.75589997e-01,\n",
       "          3.79599988e-01,  -3.63779992e-01,   1.32280007e-01,\n",
       "         -1.16079998e+00,   1.97990000e-01,  -2.09870003e-02,\n",
       "         -2.94429988e-01,  -1.38160005e-01,   3.93990017e-02,\n",
       "         -5.12719989e-01,  -6.08049989e-01,   6.85629994e-02,\n",
       "          8.46609995e-02,  -5.16820014e-01,   2.74679989e-01,\n",
       "         -2.25299999e-01,  -6.86590016e-01,   1.58329993e-01,\n",
       "          3.93189996e-01], dtype=float32), 136)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index.get('...'), t.word_index['...']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create a weight matrix for words in training docs\n",
    "embedding_matrix = zeros((vocab_size, 100))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.08139999e-01,   3.58909994e-01,   8.85179996e-01,\n",
       "        -4.43589985e-01,  -5.94860017e-01,   3.36030006e-01,\n",
       "         2.20670000e-01,  -2.28680000e-01,   3.17539990e-01,\n",
       "        -3.80080014e-01,   6.47570014e-01,  -1.06610000e-01,\n",
       "         9.07329991e-02,   4.26490009e-02,   2.32319999e-03,\n",
       "        -8.50799978e-02,  -7.65439987e-01,   6.59240007e-01,\n",
       "        -1.16489995e+00,   7.28209972e-01,   2.49039993e-01,\n",
       "         5.30610010e-02,  -3.93390000e-01,  -2.65689999e-01,\n",
       "         5.13100028e-01,   7.51519978e-01,  -1.60030007e-01,\n",
       "        -4.55449998e-01,   5.58340013e-01,  -1.86780006e-01,\n",
       "         1.38100004e-03,   4.25550014e-01,   1.31119996e-01,\n",
       "        -2.21279994e-01,  -1.83630005e-01,  -4.20720018e-02,\n",
       "         1.64600000e-01,   8.76670033e-02,  -3.97680001e-03,\n",
       "        -5.45130014e-01,  -1.36419997e-01,   5.08280005e-03,\n",
       "         1.05789997e-01,  -3.45759988e-01,  -5.86000025e-01,\n",
       "        -8.03650022e-01,   8.87759961e-03,  -1.88800007e-01,\n",
       "        -4.31169987e-01,  -1.17560005e+00,   1.45300001e-01,\n",
       "         3.06609988e-01,  -3.14619988e-01,   1.24290001e+00,\n",
       "        -3.96840006e-01,  -2.18440008e+00,   5.78069985e-01,\n",
       "         7.58010021e-04,   1.40100002e+00,   4.42699999e-01,\n",
       "        -7.61839986e-01,   6.22139990e-01,  -1.04949999e+00,\n",
       "        -5.88599980e-01,   1.20379996e+00,   3.02019995e-02,\n",
       "         5.21589994e-01,   2.65960008e-01,   2.31439993e-02,\n",
       "        -7.34770000e-02,   5.32079995e-01,  -4.83779997e-01,\n",
       "        -1.35820001e-01,  -3.54090005e-01,   8.38410020e-01,\n",
       "         1.41969994e-01,   1.11790001e-02,   2.07839996e-01,\n",
       "        -7.64609993e-01,  -9.92100015e-02,   3.75589997e-01,\n",
       "         3.79599988e-01,  -3.63779992e-01,   1.32280007e-01,\n",
       "        -1.16079998e+00,   1.97990000e-01,  -2.09870003e-02,\n",
       "        -2.94429988e-01,  -1.38160005e-01,   3.93990017e-02,\n",
       "        -5.12719989e-01,  -6.08049989e-01,   6.85629994e-02,\n",
       "         8.46609995e-02,  -5.16820014e-01,   2.74679989e-01,\n",
       "        -2.25299999e-01,  -6.86590016e-01,   1.58329993e-01,\n",
       "         3.93189996e-01])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix[136]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# np.save('word_index.npy',t.word_index)\n",
    "np.save('embedding_matrix_punc.npy',embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13668, 100)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix = np.load('embedding_matrix_punc.npy')\n",
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Sequences, Next Sequences and Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Death Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "step = 1\n",
    "seq_death = []\n",
    "next_seq_death = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "death_doc = encoded_docs[0]\n",
    "quote_len_death = len(death_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(0, quote_len_death - maxlen, step):\n",
    "    seq_death.append(death_doc[i: i + maxlen])\n",
    "    next_seq_death.append(death_doc[i + maxlen])\n",
    "\n",
    "print('nb sequences:', len(seq_death))\n",
    "\n",
    "seq_death = np.asarray(seq_death)\n",
    "next_seq_death = np.asarray(next_seq_death)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_death = np.zeros(next_seq_death.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_death"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funny Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data/funny.txt','r') as funnyfile:\n",
    "    funnyquotes = funnyfile.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_docs = t.texts_to_sequences(funnyquotes)\n",
    "funny_doc = encoded_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "step = 1\n",
    "seq_funny = []\n",
    "next_seq_funny = []\n",
    "\n",
    "quote_len_funny = len(funny_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences: 24396\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, quote_len_funny - maxlen, step):\n",
    "    seq_funny.append(funny_doc[i: i + maxlen])\n",
    "    next_seq_funny.append(funny_doc[i + maxlen])\n",
    "\n",
    "print('nb sequences:', len(seq_funny))\n",
    "\n",
    "seq_funny = np.asarray(seq_funny)\n",
    "next_seq_funny = np.asarray(next_seq_funny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_funny = np.ones(next_seq_funny.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1., ...,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_funny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.vstack([seq_death,seq_funny])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels_death = np.reshape(labels_death,(labels_death.shape[0],1))\n",
    "labels_funny  = np.reshape(labels_funny,(labels_funny.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.vstack([labels_death,labels_funny])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence Classification Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.add(LSTM(100))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=24)\n",
    "'''Train on 38108 samples, validate on 9528 samples\n",
    "Epoch 1/10\n",
    "38108/38108 [==============================] - 80s 2ms/step - \n",
    "loss: 0.0391 - acc: 0.9859 - val_loss: 4.4262e-04 - val_acc: 0.9999'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generation using Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[    6,   343,    77, ...,    57,  1419,    40],\n",
       "        [  343,    77,     5, ...,  1419,    40,    42],\n",
       "        [   77,     5,    41, ...,    40,    42,  1038],\n",
       "        ..., \n",
       "        [   73,    13, 10360, ...,  3394,  1798,    23],\n",
       "        [   13, 10360,     8, ...,  1798,    23,     1],\n",
       "        [10360,     8,     1, ...,    23,     1,    53]]),\n",
       " array([  42, 1038,   75, ...,    1,   53, 1239]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_funny, next_seq_funny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = to_categorical(next_seq_funny, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = seq_funny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "# model.add(MaxPooling1D(pool_size=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.add(LSTM(100, return_sequences=True)) #\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(y.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 100, 100)          1366200   \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 13662)             1379862   \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 13662)             0         \n",
      "=================================================================\n",
      "Total params: 2,826,462\n",
      "Trainable params: 1,460,262\n",
      "Non-trainable params: 1,366,200\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model.add(Activation('softmax'))\n",
    "optimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)\n",
    "\n",
    "model.compile(\n",
    "    loss='categorical_crossentropy',\n",
    "    optimizer=optimizer,\n",
    "    metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print_callback = LambdaCallback(on_epoch_end=on_epoch_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 19516 samples, validate on 4880 samples\n",
      "Epoch 1/10\n",
      "19516/19516 [==============================] - 130s 7ms/step - loss: 6.7342 - acc: 0.0378 - val_loss: 6.4484 - val_acc: 0.0402\n",
      "\n",
      "----- Generating text after Epoch: 0\n",
      "----- diversity: 0.2\n",
      "[52, 108, 1641, 37, 1341, 37, 10038, 475, 8, 11, 10, 154, 776, 9, 2493, 11, 52, 2, 13, 115, 4923, 64, 10, 348, 11, 37, 9, 99, 475, 228, 102, 7, 360, 7, 139, 13, 22, 2949, 22, 3149, 10039, 7, 59, 245, 863, 4010, 7, 17, 96, 3149, 35, 15, 286, 1679, 280, 4, 7, 38, 250, 10, 9, 598, 5, 1336, 4, 53, 23, 6, 114, 6535, 227, 3, 601, 4, 61, 1, 83, 11, 5, 53, 93, 10, 38, 178, 61, 100, 4, 126, 12, 890, 23, 15, 4686, 2701, 40, 11, 133, 824, 7, 290]\n",
      "----- Generating with seed: ['has', 'some', 'dramatic', 'or', 'violent', 'or', 'sinister', 'stuff', 'in', 'it', 'you', 'cant', 'forget', 'that', 'primarily', 'it', 'has', 'to', 'be', 'even', 'funnier', 'than', 'you', 'read', 'it', 'or', 'that', 'other', 'stuff', 'doesnt', 'work', 'i', 'wish', 'i', 'could', 'be', 'as', 'thin', 'as', 'jessica', 'simpson', 'i', 'think', 'she', 'looks', 'gorgeous', 'i', 'have', 'had', 'jessica', 'on', 'my', 'show', 'several', 'times', 'and', 'i', 'can', 'tell', 'you', 'that', 'girl', 'is', 'genuine', 'and', 'funny', 'with', 'a', 'great', 'selfdeprecating', 'sense', 'of', 'humor', 'and', 'at', 'the', 'time', 'it', 'is', 'funny', 'how', 'you', 'can', 'look', 'at', 'something', 'and', 'say', 'for', 'example', 'with', 'my', 'shoulder', 'injury', 'when', 'it', 'first', 'happened', 'i', 'said']\n",
      "i i i i i i i i i i i i i i i i i i i i \n",
      "Epoch 2/10\n",
      "19516/19516 [==============================] - 133s 7ms/step - loss: 6.0671 - acc: 0.0508 - val_loss: 6.4382 - val_acc: 0.0650\n",
      "\n",
      "----- Generating text after Epoch: 1\n",
      "----- diversity: 0.2\n",
      "[1019, 499, 1109, 7, 1087, 10, 96, 6, 215, 381, 4, 7, 371, 34, 1698, 75, 61, 1, 203, 4, 45, 341, 968, 7, 59, 68, 121, 2, 58, 66, 103, 93, 11, 27, 1097, 1422, 53, 2, 126, 10, 163, 33, 369, 8, 670, 21, 7, 236, 24, 48, 246, 40, 10, 213, 33, 1034, 1563, 110, 1762, 48, 1552, 24, 6, 79, 53, 281, 24, 438, 7, 20, 1015, 75, 15, 1034, 68, 6, 215, 1657, 3, 328, 165, 1575, 7, 20, 104, 1481, 3, 248, 4, 7, 59, 24, 81, 114, 2, 128, 9, 49, 19, 165]\n",
      "----- Generating with seed: ['song', 'called', 'jesus', 'i', 'heard', 'you', 'had', 'a', 'big', 'house', 'and', 'i', 'remember', 'people', 'standing', 'up', 'at', 'the', 'end', 'and', 'me', 'thinking', 'oh', 'i', 'think', 'im', 'going', 'to', 'like', 'this', 'thats', 'how', 'it', 'all', 'began', 'sounds', 'funny', 'to', 'say', 'you', 'got', 'your', 'start', 'in', 'church', 'but', 'i', 'did', 'its', 'so', 'different', 'when', 'you', 'change', 'your', 'hair', 'color', 'youre', 'treated', 'so', 'differently', 'its', 'a', 'very', 'funny', 'experience', 'its', 'fun', 'i', 'love', 'changing', 'up', 'my', 'hair', 'im', 'a', 'big', 'fan', 'of', 'certain', 'new', 'acts', 'i', 'love', 'any', 'genre', 'of', 'music', 'and', 'i', 'think', 'its', 'really', 'great', 'to', 'see', 'that', 'there', 'are', 'new']\n",
      "funny and i to the funny and i to the funny and i to the funny and i to the \n",
      "Epoch 3/10\n",
      " 1800/19516 [=>............................] - ETA: 2:00 - loss: 5.8995 - acc: 0.0678"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d5fab11b7191>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           callbacks=[print_callback])\n\u001b[0m",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/krohak/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), \n",
    "          epochs=10, \n",
    "          batch_size=24, #64\n",
    "          callbacks=[print_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_word = np.load('index_word.npy')\n",
    "index_word = index_word.item()\n",
    "\n",
    "def sample(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    # preds = preds[1:]\n",
    "    # preds = np.asarray(preds).astype('float64')\n",
    "    # preds = np.log(preds) / temperature\n",
    "    # exp_preds = np.exp(preds)\n",
    "    # preds = exp_preds / np.sum(exp_preds)\n",
    "    # probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(preds)\n",
    "\n",
    "\n",
    "def on_epoch_end(epoch, logs):\n",
    "    # Function invoked at end of each epoch. Prints generated text.\n",
    "    print()\n",
    "    print('----- Generating text after Epoch: %d' % epoch)\n",
    "\n",
    "    start_index = np.random.randint(0, len(funny_doc) - maxlen - 1)\n",
    "    for diversity in [0.2]: # 0.5, 1.0, 1.2\n",
    "        print('----- diversity:', diversity)\n",
    "\n",
    "        generated = ''\n",
    "        sentence = funny_doc[start_index: start_index + maxlen]\n",
    "        print(sentence)\n",
    "        generated.join([str([index_word[value]]).join(' ') for value in sentence])\n",
    "        print('----- Generating with seed: %s'%[index_word[word] for word in sentence])\n",
    "        #sys.stdout.write(generated)\n",
    "\n",
    "        for i in range(20):\n",
    "            x_pred = np.reshape(sentence,(1, maxlen))\n",
    "\n",
    "            preds = model.predict(x_pred, verbose=0)\n",
    "            preds = preds[0]\n",
    "            # print(preds.shape)\n",
    "            next_index = sample(preds, diversity)\n",
    "            #print(next_index)\n",
    "            next_char = index_word[next_index]\n",
    "\n",
    "            generated.join(str(next_char))\n",
    "            sentence = np.append(sentence[1:],next_index)\n",
    "\n",
    "            sys.stdout.write(next_char)\n",
    "            sys.stdout.write(\" \")\n",
    "            sys.stdout.flush()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
